{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6769b025-ad49-4fd0-93a7-9eb13656026a",
   "metadata": {},
   "source": [
    "# Do this once - Spark initialization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3dbc94-a2b4-4ddd-b497-7bd673154f01",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-03-13T12:51:23.986710Z",
     "start_time": "2024-03-13T12:51:21.020331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /usr/local/spark/python (3.5.0)\r\n",
      "Collecting ipython-sql\r\n",
      "  Downloading ipython_sql-0.5.0-py3-none-any.whl.metadata (17 kB)\r\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.11/site-packages (2.0.22)\r\n",
      "Collecting delta-spark==3.0.0\r\n",
      "  Downloading delta_spark-3.0.0-py3-none-any.whl.metadata (2.0 kB)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\r\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.0)\r\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark==3.0.0) (6.8.0)\r\n",
      "Collecting prettytable (from ipython-sql)\r\n",
      "  Downloading prettytable-3.10.0-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from ipython-sql) (8.16.1)\r\n",
      "Collecting sqlparse (from ipython-sql)\r\n",
      "  Downloading sqlparse-0.4.4-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from ipython-sql) (1.16.0)\r\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.11/site-packages (from ipython-sql) (0.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (4.8.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy) (3.0.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark==3.0.0) (3.17.0)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (3.0.39)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (2.16.1)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (0.6.2)\r\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (5.11.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->ipython-sql) (4.8.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prettytable->ipython-sql) (0.2.8)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipython-sql) (1.2.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipython-sql) (2.4.0)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->ipython-sql) (0.2.2)\r\n",
      "Downloading delta_spark-3.0.0-py3-none-any.whl (21 kB)\r\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m200.5/200.5 kB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading ipython_sql-0.5.0-py3-none-any.whl (20 kB)\r\n",
      "Downloading prettytable-3.10.0-py3-none-any.whl (28 kB)\r\n",
      "Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.2/41.2 kB\u001B[0m \u001B[31m5.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: py4j, sqlparse, prettytable, delta-spark, ipython-sql\r\n",
      "Successfully installed delta-spark-3.0.0 ipython-sql-0.5.0 prettytable-3.10.0 py4j-0.10.9.7 sqlparse-0.4.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.5.0 ipython-sql sqlalchemy delta-spark==3.0.0 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5d055f-2964-4019-935c-110149d23cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:05:11.404510Z",
     "start_time": "2024-03-13T16:05:11.224148Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, min, count\n",
    "\n",
    "def configure_spark_for_s3(spark: SparkSession):\n",
    "    connection_time_out = \"600000\"\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.endpoint\", os.environ[\"MINIO_URL\"]\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.access.key\", os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.secret.key\", os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.connection.timeout\", connection_time_out\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"spark.sql.debug.maxToStringFields\", \"100\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.path.style.access\", \"true\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.connection.ssl.enabled\", \"false\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_spark():\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .master(\"local\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    configure_spark_for_s3(spark)\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f89e400-ba03-4394-a73b-98211faa2d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:05:40.900701Z",
     "start_time": "2024-03-13T16:05:14.246358Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = create_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd520583-1825-4a8e-ad2a-cb65eefc7251",
   "metadata": {},
   "source": [
    "# RECORD GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d74689-a691-4303-b095-acf8fb7b31bc",
   "metadata": {},
   "source": [
    "## Functions to generate a JSON dataset to pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4f7296-01f3-4696-813c-d41dd0797e3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:06:07.160218Z",
     "start_time": "2024-03-13T16:06:07.144843Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Method to return a random User ID between 1 and 10 (set low for testing some stateful streaming aggregations, higher for more variability)\n",
    "def returnUserId():\n",
    "  return random.randint(1, 6)\n",
    "\n",
    "# Return a random float value for different purposes, rounded to 4 places\n",
    "def returnValue():\n",
    "  return round(random.uniform(111.1111, 9999999999.9999), 4)\n",
    "\n",
    "# Method to return a string of random characters - hard-coded to length of 30\n",
    "def returnString():\n",
    "  letters = string.ascii_letters\n",
    "  return ( ''.join(random.choice(letters) for i in range(30)) )\n",
    "\n",
    "def returnTransactionTimestamp():\n",
    "  currentDateTime = datetime.now()\n",
    "  return currentDateTime.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "# Generate a record\n",
    "def generateRecord():\n",
    "  return (returnUserId(), returnString(), returnValue(), returnValue(), returnValue(), returnTransactionTimestamp())\n",
    "  \n",
    "# Generate a list of records\n",
    "def generateRecordSet(recordCount):\n",
    "  recordSet = []\n",
    "  for x in range(recordCount):\n",
    "    recordSet.append(generateRecord())\n",
    "  return recordSet\n",
    "\n",
    "# Generate a set of data, convert it to a Dataframe, write it out as one json file in a temp location, \n",
    "# move the json file to the desired location that the Auto Loader will be watching and then delete the temp location\n",
    "def writeJsonFile(recordCount):\n",
    "  recordColumns = [\"userId\", \"stringCode\", \"value1\", \"value2\", \"value3\", \"transactionTimestamp\"]\n",
    "  recordSet = generateRecordSet(recordCount)\n",
    "  recordDf = spark.createDataFrame(data=recordSet, schema=recordColumns)\n",
    "  \n",
    "  # Write out the json file with Spark in a temp location - this will create a directory with the file we want the Auto Loader to\n",
    "  # pick up underneath it\n",
    "  #recordDf.coalesce(1).write.format(\"json\").save(tempPath)\n",
    "  \n",
    "  # Grab the file from the temp location, write it to the location we want and then delete the temp directory\n",
    "  #tempJson = os.path.join(tempPath, dbutils.fs.ls(tempPath)[3][1])\n",
    "  #dbutils.fs.cp(tempJson, destinationPath)\n",
    "  #dbutils.fs.rm(tempPath, True)\n",
    "\n",
    "  \n",
    "  #recordDf.write.format(\"delta\").save(\"s3a://silver/users\")\n",
    "  recordDf.write.mode(\"append\").format(\"delta\").save(\"s3a://silver/users\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2911f-d45c-410a-9fe3-763553ee7f46",
   "metadata": {},
   "source": [
    "# Define Record Count, Temporary Location, Auto Loader-Monitored Location and Sleep Interval Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c822efb7-5308-405e-b11a-81f74eb88b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:07:24.381642Z",
     "start_time": "2024-03-13T16:07:24.365804Z"
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# Assuming your data generation and writeJsonFile functions are defined in previous cells or in the same cell\n",
    "\n",
    "def continuous_data_write(recordCount, stop_event):\n",
    "    while not stop_event.is_set():\n",
    "        writeJsonFile(recordCount)  # Ensure this matches the definition of writeJsonFile\n",
    "        time.sleep(sleepIntervalSeconds)\n",
    "\n",
    "# Define parameters for the data generation\n",
    "recordCount = 5\n",
    "sleepIntervalSeconds = 1\n",
    "destinationPath = \"s3a://silver/users\"\n",
    "\n",
    "# Create a stop event\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Start the background thread\n",
    "data_thread = threading.Thread(target=continuous_data_write, args=(recordCount, stop_event))\n",
    "data_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdc6613-bc2b-4cf9-910a-04ca6fa13f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:06:19.094291Z",
     "start_time": "2024-03-13T16:06:19.067282Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_event' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# To stop the data generation\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mstop_event\u001B[49m\u001B[38;5;241m.\u001B[39mset()\n\u001B[1;32m      3\u001B[0m data_thread\u001B[38;5;241m.\u001B[39mjoin()  \u001B[38;5;66;03m# Ensure the thread has finished\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'stop_event' is not defined"
     ]
    }
   ],
   "source": [
    "# To stop the data generation\n",
    "stop_event.set()\n",
    "data_thread.join()  # Ensure the thread has finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "105fe2f7-fe3b-44d9-ad2a-f4eb1394385f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:11.145017Z",
     "start_time": "2024-03-13T16:07:29.106778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m sleepIntervalSeconds \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m----> 6\u001B[0m   \u001B[43mwriteJsonFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecordCount\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m   time\u001B[38;5;241m.\u001B[39msleep(sleepIntervalSeconds)\n",
      "Cell \u001B[0;32mIn[3], line 53\u001B[0m, in \u001B[0;36mwriteJsonFile\u001B[0;34m(recordCount)\u001B[0m\n\u001B[1;32m     40\u001B[0m recordDf \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m=\u001B[39mrecordSet, schema\u001B[38;5;241m=\u001B[39mrecordColumns)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Write out the json file with Spark in a temp location - this will create a directory with the file we want the Auto Loader to\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# pick up underneath it\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m#recordDf.coalesce(1).write.format(\"json\").save(tempPath)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     51\u001B[0m \n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m#recordDf.write.format(\"delta\").save(\"s3a://silver/users\")\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m \u001B[43mrecordDf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43ms3a://silver/users\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1463\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1461\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1463\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "recordCount=5\n",
    "# tempPath = \"s3://my/path/here/temp\"\n",
    "sleepIntervalSeconds = 1\n",
    "\n",
    "while True:\n",
    "  writeJsonFile(recordCount)\n",
    "  time.sleep(sleepIntervalSeconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "896853c8-b025-45e6-9260-9e80fef5b267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T13:07:58.821846Z",
     "start_time": "2024-03-13T13:07:58.104660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     userId                      stringCode        value1        value2  \\\n0         6  zLwpeBrIXpdDxguPytJdzskbZZcuGj  2.329014e+08  9.562096e+09   \n1         5  DVsWzAuzWbcFVMJvwZXFfnDHzaFLFH  7.758462e+09  5.586205e+09   \n2         5  rikINBzelrOaQSrefDbkEpdgdZNmPc  4.799428e+09  2.648775e+09   \n3         2  DAQJzEOCqhYMULxfwEemAtJpqxlHui  9.954533e+09  6.120536e+09   \n4         3  awTOHjBiKDqglMKqNUGQCPPWxwwiRQ  4.247559e+09  4.874784e+09   \n..      ...                             ...           ...           ...   \n535       1  rkgazhEfOsQDRwyxdKEMuUOdgIOJsl  2.178721e+08  5.362222e+09   \n536       4  FLsgKtfgVcIuPOsjQmbIxGrpTUhguh  7.982872e+09  7.151681e+07   \n537       5  OsZalcfOCYTuFTGZHMLzrKtBLOqGlL  1.301974e+09  3.290370e+09   \n538       2  bfidqLhzfkfNztiXLyPbFjZdhhmpSa  6.754345e+09  1.898216e+09   \n539       6  exmKFAVeFKLkwUwvoNTGJAFXqVGRyH  5.645155e+09  3.180189e+09   \n\n           value3        transactionTimestamp  \n0    6.677291e+09  2024-03-13 13:06:13.709474  \n1    9.793474e+09  2024-03-13 13:06:13.710086  \n2    2.412623e+09  2024-03-13 13:06:13.710198  \n3    3.858808e+09  2024-03-13 13:06:13.710265  \n4    3.916491e+09  2024-03-13 13:06:13.710417  \n..            ...                         ...  \n535  4.471909e+09  2024-03-13 13:03:34.541247  \n536  8.475935e+09  2024-03-13 13:03:34.541321  \n537  4.548687e+08  2024-03-13 13:03:34.541347  \n538  5.684968e+09  2024-03-13 13:03:34.541371  \n539  8.333530e+08  2024-03-13 13:03:34.541394  \n\n[540 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>stringCode</th>\n      <th>value1</th>\n      <th>value2</th>\n      <th>value3</th>\n      <th>transactionTimestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>zLwpeBrIXpdDxguPytJdzskbZZcuGj</td>\n      <td>2.329014e+08</td>\n      <td>9.562096e+09</td>\n      <td>6.677291e+09</td>\n      <td>2024-03-13 13:06:13.709474</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>DVsWzAuzWbcFVMJvwZXFfnDHzaFLFH</td>\n      <td>7.758462e+09</td>\n      <td>5.586205e+09</td>\n      <td>9.793474e+09</td>\n      <td>2024-03-13 13:06:13.710086</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>rikINBzelrOaQSrefDbkEpdgdZNmPc</td>\n      <td>4.799428e+09</td>\n      <td>2.648775e+09</td>\n      <td>2.412623e+09</td>\n      <td>2024-03-13 13:06:13.710198</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>DAQJzEOCqhYMULxfwEemAtJpqxlHui</td>\n      <td>9.954533e+09</td>\n      <td>6.120536e+09</td>\n      <td>3.858808e+09</td>\n      <td>2024-03-13 13:06:13.710265</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>awTOHjBiKDqglMKqNUGQCPPWxwwiRQ</td>\n      <td>4.247559e+09</td>\n      <td>4.874784e+09</td>\n      <td>3.916491e+09</td>\n      <td>2024-03-13 13:06:13.710417</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>535</th>\n      <td>1</td>\n      <td>rkgazhEfOsQDRwyxdKEMuUOdgIOJsl</td>\n      <td>2.178721e+08</td>\n      <td>5.362222e+09</td>\n      <td>4.471909e+09</td>\n      <td>2024-03-13 13:03:34.541247</td>\n    </tr>\n    <tr>\n      <th>536</th>\n      <td>4</td>\n      <td>FLsgKtfgVcIuPOsjQmbIxGrpTUhguh</td>\n      <td>7.982872e+09</td>\n      <td>7.151681e+07</td>\n      <td>8.475935e+09</td>\n      <td>2024-03-13 13:03:34.541321</td>\n    </tr>\n    <tr>\n      <th>537</th>\n      <td>5</td>\n      <td>OsZalcfOCYTuFTGZHMLzrKtBLOqGlL</td>\n      <td>1.301974e+09</td>\n      <td>3.290370e+09</td>\n      <td>4.548687e+08</td>\n      <td>2024-03-13 13:03:34.541347</td>\n    </tr>\n    <tr>\n      <th>538</th>\n      <td>2</td>\n      <td>bfidqLhzfkfNztiXLyPbFjZdhhmpSa</td>\n      <td>6.754345e+09</td>\n      <td>1.898216e+09</td>\n      <td>5.684968e+09</td>\n      <td>2024-03-13 13:03:34.541371</td>\n    </tr>\n    <tr>\n      <th>539</th>\n      <td>6</td>\n      <td>exmKFAVeFKLkwUwvoNTGJAFXqVGRyH</td>\n      <td>5.645155e+09</td>\n      <td>3.180189e+09</td>\n      <td>8.333530e+08</td>\n      <td>2024-03-13 13:03:34.541394</td>\n    </tr>\n  </tbody>\n</table>\n<p>540 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://silver/users\")\n",
    "# display(df.select(count(\"*\")).toPandas())\n",
    "display(df.select(\"*\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2139e96fe033179d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
