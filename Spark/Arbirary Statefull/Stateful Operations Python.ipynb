{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea88fe06-a235-4bcb-a30f-02a0daa116bf",
   "metadata": {},
   "source": [
    "## This is the Python implementation of the example for Arbitrary Stateful Operations\n",
    "### The use case description - transaction count within the last five minutes:\n",
    "\n",
    "* When a transaction record is received for a user, the count of transactions for that user that occurred within the last 5 minutes of the transaction time is calculated and written to a table.\n",
    "* Only the current count for a given user is kept. If no transactions are received for a user, the count should automatically go down.  An ML model uses the count to determine if too many have occurred within the last 5 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd7130-364c-4f07-ba75-e42809675221",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5.0 ipython-sql sqlalchemy delta-spark==3.0.0 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a77c2eb-fda3-486a-8dde-830f0e9f294a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:20.159920Z",
     "start_time": "2024-03-13T16:08:19.976951Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, min, count\n",
    "\n",
    "def configure_spark_for_s3(spark: SparkSession):\n",
    "    connection_time_out = \"600000\"\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.endpoint\", os.environ[\"MINIO_URL\"]\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.access.key\", os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.secret.key\", os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.connection.timeout\", connection_time_out\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"spark.sql.debug.maxToStringFields\", \"100\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.path.style.access\", \"true\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.connection.ssl.enabled\", \"false\"\n",
    "    )\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_spark():\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .master(\"local\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    configure_spark_for_s3(spark)\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f445b609-55b0-4799-af49-5ee90066c6b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:24.586862Z",
     "start_time": "2024-03-13T16:08:22.017743Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = create_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff419aea-a359-40fd-88b7-ad3be39ff049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:27.489824Z",
     "start_time": "2024-03-13T16:08:27.208644Z"
    }
   },
   "outputs": [],
   "source": [
    "# These can also be in the cluster settings - they will automatically compact sets of small files into larger files as the stream writes to Delta for more optimal read performance\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "# This setting will automatically allow schema evolution of the target Delta table with merge statements\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5713dd-61da-4dad-bd8f-6ffcc7c70670",
   "metadata": {},
   "source": [
    "### Define the output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68204969-1e09-4e55-8d6e-35ad497f06d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:36.258158Z",
     "start_time": "2024-03-13T16:08:30.035766Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "# Define the schema based on your table definition\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", LongType(), True),\n",
    "    StructField(\"purchaseCount\", IntegerType(), True),\n",
    "    StructField(\"eventTimestamp\", TimestampType(), True),\n",
    "    StructField(\"isTimeout\", BooleanType(), True),\n",
    "    StructField(\"stateList\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Specify the path for the Delta table\n",
    "deltaTablePath = \"s3a://silver/aggregationtable\"\n",
    "\n",
    "# Use DataFrame API to create an empty DataFrame with the schema\n",
    "emptyDF = spark.createDataFrame([], schema)\n",
    "\n",
    "# Save the empty DataFrame as a Delta table at the specified path\n",
    "# Note: This operation creates the table if it does not exist, or does nothing if it already exists\n",
    "emptyDF.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"ignore\") \\\n",
    "    .option(\"path\", deltaTablePath) \\\n",
    "    .saveAsTable(\"aggregationtable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebae7c-dfa3-452f-b8ad-4976d9f927b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- This is creating the database and table in the metastore if they don't already exist \n",
    "# create database if not exists streamtest;\n",
    "# create table if not exists streamtest.aggregationtablePython (userId Long, purchaseCount Int, eventTimestamp Timestamp, isTimeout Boolean, stateList String)\n",
    "# using delta \n",
    "# location 's3://my/path/here/aggregationtablePython'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747b99a-47ac-451c-83f7-28430aa332f4",
   "metadata": {},
   "source": [
    "### Define applyInPandasWithState logic\n",
    "* This is the Python version of arbitrary stateful operations\n",
    "* This logic will keep track of all the transactions that occurred in the previous 5 minutes for a given user and update the count every time new transactions are received for that user  \n",
    "* If no transactions have been received after 1 minute for a given user, the logic will still emit a count and will remove records from state that are more than 5 minutes old\n",
    "* If the stream has no data coming through at all then nothing will be updated.  Something must be coming through the stream for this logic to be executed\n",
    "* The tranactionCountMinutes and maxRecordIntervalMinutes variables can be updated below to change how far back in time to count records and how often a new count will be emitted if no new records for a user are received\n",
    "* Check https://www.databricks.com/blog/2022/10/18/python-arbitrary-stateful-processing-structured-streaming.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8cc0e9-51f5-48d9-8e91-53233ec585d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:36.269020Z",
     "start_time": "2024-03-13T16:08:36.259039Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import namedtuple\n",
    "from typing import Tuple, Iterator, List\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "# How far back in minutes to count transactions.  This will be used to calculate when a record should be removed from the state and no longer counted\n",
    "transactionCountMinutes = 5\n",
    "\n",
    "# The maximum amount of minutes to wait before emitting a record\n",
    "maxRecordIntervalMinutes = 1\n",
    "\n",
    "# Documented for completeness - the expected structure of the input\n",
    "#inputSchema = \"userId LONG, transactionTimestamp TIMESTAMP\"\n",
    "\n",
    "# The schema for the state - this is what the stream is storing so that it can count the number of transactions for a user within the last 5 minutes\n",
    "# The fields are the latest timestamp that was received for this key (user) and a list of transaction timestamps received for this key\n",
    "# Examples of both StructType syntax and DDL\n",
    "#stateSchema =  StructType([StructField(\"latestTimestamp\", TimestampType()), StructField(\"currentPurchases\", ArrayType(TimestampType()))])\n",
    "stateSchema = \"latestTimestamp TIMESTAMP, currentPurchases ARRAY<TIMESTAMP>\"\n",
    "\n",
    "# The schema in DDL form for the values being emitted - the key (user), the count of purchases in the last 5 minutes, the event datetime that triggered this update, a boolean \n",
    "# indicating whether the update was triggered by a timeout, meaning a record wasn't received for the user within a minute, and a list of all the timestamps in state for this key\n",
    "# in the form of a string for debugging purposes\n",
    "outputSchema = \"userId LONG, purchaseCount INT, eventTimestamp TIMESTAMP, isTimeout BOOLEAN, stateList STRING\"\n",
    "\n",
    "# A named tuple with the structure of the state.  Since the state is referred to across multiple functions in this case, a named tuple will make the rest of the code more readable\n",
    "State = namedtuple(\"State\", \"latestTimestamp currentPurchases\")\n",
    "\n",
    "# A function that will remove records that are more than 5 minutes old from the state\n",
    "# Parameter types that are expected: datetime, List(datetime)\n",
    "# Returns the latest timestamp and the list of transaction timestamps remaining in state as a State tuple\n",
    "def removeExpiredRecords(newLatestTimestamp: datetime, currentPurchases: List[datetime]):\n",
    "  # Calculate the state expiration timestamp - the latest timestamp minus the transaction count minutes\n",
    "  expirationTimestamp = newLatestTimestamp - timedelta(minutes=transactionCountMinutes)\n",
    "  \n",
    "  # If there are records in state, loop through the list of current purchases and remove any that are less than the expiration timestamp\n",
    "  newPurchaseList: List[datetime] = []\n",
    "  if currentPurchases:\n",
    "    for purchase in currentPurchases: \n",
    "      if (purchase >= expirationTimestamp):\n",
    "        newPurchaseList.append(purchase)\n",
    "    \n",
    "  return State(newLatestTimestamp, newPurchaseList)\n",
    "\n",
    "# A function that adds new records to the state\n",
    "# Parameter types that are expected: the new records as a list of datetime, the current state object as a State tuple\n",
    "# Returns the new latest timestamp and the updated list of purchases as a State tuple\n",
    "def addNewRecords(newRecords: List[datetime], purchaseCountState):\n",
    "  # Get the latest timestamp in the set of new records\n",
    "  newLatestTimestamp: datetime = max(newRecords)\n",
    "  \n",
    "  # Compare to the latestTimestamp in the purchaseCountState, use whichever is greater\n",
    "  # This is in case we've received data out of order\n",
    "  if (newLatestTimestamp < purchaseCountState.latestTimestamp):\n",
    "    newLatestTimestamp = purchaseCountState.latestTimestamp \n",
    "  \n",
    "  # Return the updated state, with the latest timestamp and the updated list of purchases\n",
    "  return State(newLatestTimestamp, purchaseCountState.currentPurchases + newRecords)\n",
    "\n",
    "\n",
    "# This is the function that is called with applyInPandasWithState.  It keeps track of the last 5 minutes of records for each key so that each time new data is received \n",
    "# it can count the number of transactions that occurred in the last 5 minuts.\n",
    "# This function will be called in two ways -\n",
    "#   If one or more records for a given user are received.  In that case it will add those records to the state, remove any records that are older than 5 minuts from the state and calculate the count\n",
    "#   If no records are received for a given user within a minute since the last time this function was called.  In that case it will remove any records that are older than 5 minutes from the state and calculate the count\n",
    "def updateState (\n",
    "  key: Tuple[int],  # This is the key we are grouping on.  It can be a tuple of one or more fields\n",
    "  values: Iterator[pd.DataFrame],  # These are the records coming into the function\n",
    "  state: GroupState # The state we're storing in-between microbatches\n",
    ") -> Iterator[pd.DataFrame]:   # The records we're outputting\n",
    "\n",
    "  # If we haven't timed out then there are values for this key\n",
    "  if not state.hasTimedOut:\n",
    "    # There can be one or more records for this key.  Iterate through them and put the transaction timestamps into a list\n",
    "    # Our input in this use case is rows of userId and transactionTimestamp\n",
    "    transactionList: List[datetime] = []\n",
    "    for value in values:\n",
    "      transactionList = transactionList + value[\"transactionTimestamp\"].tolist()\n",
    "\n",
    "    # Now get the previous state if it exists.  If it doesn't exist (if this is the first time we've received a record for this user the state won't exist yet) then set the initial state to the \n",
    "    # maximum transactionTimestamp from the input list and an empty List of datetime\n",
    "    maxTimestamp: datetime = max(transactionList)\n",
    "    prevState = State(maxTimestamp, [])\n",
    "    if state.exists:\n",
    "      (latestTimestamp, currentPurchases) = state.get\n",
    "      prevState = State(latestTimestamp, currentPurchases)\n",
    "    \n",
    "    # Add the new records to the state\n",
    "    stateWithNewRecords = addNewRecords(transactionList, prevState)\n",
    "    \n",
    "    # Remove expired records from the state\n",
    "    # After this function only the transactions that occurred within the last five minutes from the latest transaction will be in the state object\n",
    "    stateWithRecordsRemoved = removeExpiredRecords(stateWithNewRecords.latestTimestamp, stateWithNewRecords.currentPurchases)\n",
    "    \n",
    "    # Save the state\n",
    "    state.update(stateWithRecordsRemoved)\n",
    "    \n",
    "    # When no data has been seen for a period of time for a given key, this timeout will trigger the else clause below\n",
    "    # The timeout will only trigger after the watermark has moved past this timestamp.  So for example if we're allowing data to be up to 30 seconds late,\n",
    "    # then this timeout will trigger at the configured timestamp plus 30 seconds\n",
    "    # Since this is our steady-state logic and we have new records for this key, set the timeout to the latest transactionTimestamp that's in state plus 30 seconds.  If no data is seen\n",
    "    # for this key for 30 seconds past the latest transactionTimestamp in the state plus the watermark time, then this function will be triggered for this key to remove expired records and emit a count\n",
    "    # Since the watermark is set at 30 seconds then this timeout will trigger approximately once per minute\n",
    "    # This converts the latest timestamp in state to milliseconds and then adds 30 seconds.  In Python setTimeoutTimestamp takes a millisecond value\n",
    "    timeoutMs = int((stateWithRecordsRemoved.latestTimestamp.timestamp() *1000) + 30000)\n",
    "    state.setTimeoutTimestamp(timeoutMs)\n",
    "\n",
    "    # Create the output record and return - the key (user), count of transactions in the last 5 minutes, the latest timestamp and a boolean indicating this record was not triggered by a timeout\n",
    "    # That comma isn't a typo - since the key in this case is a tuple with just one value, you need the hanging comma to get the value instead of the object\n",
    "    (userId,) = key\n",
    "    purchaseCount = len(stateWithRecordsRemoved.currentPurchases)\n",
    "    eventTimestamp = stateWithRecordsRemoved.latestTimestamp\n",
    "    isTimeout = False\n",
    "\n",
    "    # Get the current list of timestamps in state as a string and return as part of the output for debugging purposes\n",
    "    stateList = []\n",
    "    for purchase in stateWithRecordsRemoved.currentPurchases:\n",
    "      stateList.append(purchase.strftime('%Y-%m-%dT%H:%M:%S.%f'))\n",
    "    stateListString = \",\".join(stateList)\n",
    "\n",
    "    # Using yield here will return a generator object for this Pandas Dataframe\n",
    "    yield pd.DataFrame({\"userId\": [userId], \"purchaseCount\": [purchaseCount], \"eventTimestamp\": [eventTimestamp], \"isTimeout\": [isTimeout], \"stateList\": [stateListString]})\n",
    "\n",
    "  else:\n",
    "    # Since a timeout was triggered that means there was no input for this key\n",
    "    # Use now as the new maximum timestamp for the state\n",
    "    (currentLatestTimestamp, currentPurchases) = state.get\n",
    "    newTimestamp = datetime.now()\n",
    "\n",
    "    # Remove expired records from the state if there are any\n",
    "    stateWithRecordsRemoved = State(newTimestamp, [])\n",
    "    if currentPurchases:\n",
    "      # After this function only the transactions that occurred within the last five minutes from the latest transaction will be in state\n",
    "      stateWithRecordsRemoved = removeExpiredRecords(newTimestamp, currentPurchases)\n",
    "    \n",
    "    # *** From this point on this is an ineficient implementation - it blindly updates the state and sets up a new timeout even when there are no records left in state, which means it will keep calling this function every minute even if\n",
    "    # *** no new records are received for a user for a long time.  Instead, you can add a conditional that removes the state completely for the user once it has no transactions left to count\n",
    "    # *** and then don't set a new timeout.  When a record for that user is received again in the future, it'll go through the normal initialization logic\n",
    "\n",
    "    # Save the new state\n",
    "    state.update(stateWithRecordsRemoved)\n",
    "\n",
    "    # Set the new timeout to now plus 30 seconds.  If no data is seen for this key in the next 30 seconds plus the watermark then this function will be triggered again to remove expired records and emit a count\n",
    "    # Since the watermark is set at 30 seconds then this timeout will trigger approximately once per minute\n",
    "    # This converts the current time to milliseconds and then adds 30 seconds.  In Python setTimeoutTimestamp takes a millisecond value\n",
    "    timeoutMs = int((newTimestamp.timestamp() *1000) + 30000)\n",
    "    state.setTimeoutTimestamp(timeoutMs)\n",
    "\n",
    "    # Create the output record and return - the key (user), count of transactions in the last 5 minutes, the latest timestamp and a boolean indicating this record was triggered by a timeout\n",
    "    # If there were no transactions left in state then the count will be returned as 0\n",
    "    (userId,) = key\n",
    "    purchaseCount = len(stateWithRecordsRemoved.currentPurchases)\n",
    "    eventTimestamp = stateWithRecordsRemoved.latestTimestamp\n",
    "    isTimeout = True\n",
    "    \n",
    "     # Get the current list of timestamps as a string and return as part of the state for debugging purposes\n",
    "    stateList = []\n",
    "    for purchase in stateWithRecordsRemoved.currentPurchases:\n",
    "      stateList.append(purchase.strftime('%Y-%m-%dT%H:%M:%S.%f'))\n",
    "    stateListString = \",\".join(stateList)\n",
    "\n",
    "    # Generate and return the output.  Note it is valid to return an empty Pandas Dataframe here\n",
    "    yield pd.DataFrame({\"userId\": [userId], \"purchaseCount\": [purchaseCount], \"eventTimestamp\": [eventTimestamp], \"isTimeout\": [isTimeout], \"stateList\": [stateListString]})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b59b5d-c4e0-444c-9f68-9ac5fb838ed0",
   "metadata": {},
   "source": [
    "### Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03fde9c-7328-4388-8cb5-deb364f81b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:40.665147Z",
     "start_time": "2024-03-13T16:08:40.213175Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# The schema for the incomming records.  Only needed if the Auto Loader's schema inference feature is not being used\n",
    "# testSchema = StructType([StructField(\"stringCode\", StringType(), True),\n",
    "#                  StructField(\"transactionTimestamp\", StringType(), True),\n",
    "#                  StructField(\"userId\", LongType(), True),\n",
    "#                  StructField(\"value1\", DoubleType(), True),\n",
    "#                  StructField(\"value2\", DoubleType(), True),\n",
    "#                  StructField(\"value3\", DoubleType(), True)\n",
    "# ])\n",
    "\n",
    "# # Define the Delta table path\n",
    "# inputPath = \"s3a://silver/users\"\n",
    "\n",
    "# # Read the Delta table as a stream\n",
    "# testInputDf = spark.read \\\n",
    "#     .format(\"json\") \\\n",
    "#     .schema(testSchema) \\\n",
    "#     .option(\"path\", inputPath) \\\n",
    "#     .option(\"maxFilesPerTrigger\", 1) \\\n",
    "#     .load() \\\n",
    "#     .selectExpr(\"userId\", \"cast(transactionTimestamp as timestamp) transactionTimestamp\")\n",
    "\n",
    "# display(testInputDf.select(\"*\").toPandas())\n",
    "\n",
    "\n",
    "inputPath = \"s3a://silver/users\"\n",
    "\n",
    "# Read the Delta table as a stream\n",
    "testInputDf = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(inputPath) \\\n",
    "    .selectExpr(\"userId\", \"cast(transactionTimestamp as timestamp) transactionTimestamp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f07350d2-f462-456a-a323-cafd6c7a83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read from cloud storage using the Auto Loader\n",
    "# # There are two ways to use the Auto Loader - Directory listing and Notifications.  Directory listing is the default, and only requires permissions on the cloud bucket that you want to read\n",
    "# # See https://docs.databricks.com/ingestion/auto-loader/index.html for documentation\n",
    "# testInputDf = (\n",
    "#   spark.readStream\n",
    "#     .format(\"cloudFiles\")\n",
    "#     .option(\"cloudFiles.format\", \"json\")\n",
    "#     .option(\"header\", \"true\")\n",
    "#     .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "#     .option(\"cloudFiles.validateOptions\", \"true\")\n",
    "#     .schema(testSchema)\n",
    "#     .load(autoloaderIngest)\n",
    "#     .selectExpr(\"userId\", \"cast(transactionTimestamp as timestamp) transactionTimestamp\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4151408-e210-4c66-9130-d9bd382dca39",
   "metadata": {},
   "source": [
    "### Call applyInPandasWithState to calculate the count of records that occurred in the last 5 minutes for each user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db5b2b3-de3f-45b0-9e45-e8c167ef4355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:44.873199Z",
     "start_time": "2024-03-13T16:08:43.540127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notice the watermark, which is required since we're using an event time-based timeout.  We're allowing incoming data to be 30 seconds late before it is dropped\n",
    "# The name of the function that was defined above is passed to the applyInPandasWithState call, along with the DDL schemas for the state and the output, the output mode and the type of timeout\n",
    "# The timeout is optional - if you don't need to use them then specify GroupStateTimeout.NoTimeout as the timeout parameter\n",
    "applyInPandasWithStateResultDf = (\n",
    "  testInputDf\n",
    "    .withWatermark(\"transactionTimestamp\", \"30 seconds\")\n",
    "    .groupBy(testInputDf[\"userId\"])\n",
    "    .applyInPandasWithState(updateState, outputSchema, stateSchema, \"append\", GroupStateTimeout.EventTimeTimeout)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88894185-67b1-41e2-ba44-9f0c171eb2ae",
   "metadata": {},
   "source": [
    "### Write out the results - merge into a Delta table with the latest counts per user using the foreachBatch sink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f27895-7cce-49cc-b767-33aa6e386f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:47.144443Z",
     "start_time": "2024-03-13T16:08:47.118777Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aggregationTable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43maggregationTable\u001B[49m\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[1;32m      2\u001B[0m      newCountsDf\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;124m\"\u001B[39m), \n\u001B[1;32m      3\u001B[0m      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mm.userId = t.userId\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m    \u001B[38;5;241m.\u001B[39mwhenMatchedUpdateAll() \\\n\u001B[1;32m      5\u001B[0m    \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll() \\\n\u001B[1;32m      6\u001B[0m    \u001B[38;5;241m.\u001B[39mexecute()\n\u001B[1;32m      8\u001B[0m display(aggregationTable\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mtoPandas())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'aggregationTable' is not defined"
     ]
    }
   ],
   "source": [
    " aggregationTable.alias(\"t\").merge(\n",
    "      newCountsDf.alias(\"m\"), \n",
    "      \"m.userId = t.userId\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "display(aggregationTable.select(\"*\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b32216-3fdb-4684-9af4-201aae4adbed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:08:51.961993Z",
     "start_time": "2024-03-13T16:08:51.763577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.streaming.query.StreamingQuery at 0xffff528ff510>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Function for foreachBatch to update the counts in the Delta table\n",
    "def updateCounts(newCountsDf, ephoch_id):\n",
    "  # Get the target Delta table that is being merged\n",
    "  aggregationTable = DeltaTable.forName(spark, \"aggregationtable\")\n",
    "  \n",
    "  # Merge the new records into the target Delta table.  This can be done with SQL syntax as well\n",
    "  aggregationTable.alias(\"t\").merge(\n",
    "      newCountsDf.alias(\"m\"), \n",
    "      \"m.userId = t.userId\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "\n",
    "# Checkpoint location for the transaction count Delta table\n",
    "aggregationCheckpointPath = \"s3a://silver/checkpoints/aggregationtable\"\n",
    "\n",
    "# Save the applyInPandasWithState result to a Delta table.  The foreachBatch sink is used so that the Delta merge can be executed as a batch operation\n",
    "# Always define a checkpoint location, and always name your stream so it shows up with a readable name in the Spark UI.\n",
    "# Yes I know I forgot to change the queryName from the Scala implementation :) \n",
    "applyInPandasWithStateResultDf.writeStream \\\n",
    "  .foreachBatch(updateCounts) \\\n",
    "  .option(\"checkpointLocation\", aggregationCheckpointPath) \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .queryName(\"testFlatMapGroups\") \\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6a4a2-ff9d-446a-b8f0-da9836a668a2",
   "metadata": {},
   "source": [
    "### Query the data while the stream is running to see the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb2ea6-ef6a-457f-a44b-b050df201126",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- You can also do a readStream from the Delta path in Python and call display\n",
    "select * from streamtest.aggregationtablePython\n",
    "order by userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b31c8a6-7379-4917-93f0-20e6632a2d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:55:38.000592Z",
     "start_time": "2024-03-13T15:55:37.563106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [userId, purchaseCount, eventTimestamp, isTimeout, stateList]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>purchaseCount</th>\n      <th>eventTimestamp</th>\n      <th>isTimeout</th>\n      <th>stateList</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Delta table from a path\n",
    "deltaTablePath = \"s3a://silver/aggregationtable\"\n",
    "df = spark.read.format(\"delta\").load(deltaTablePath)\n",
    "#df.printSchema()\n",
    "\n",
    "# Then, order by userId and display as before\n",
    "#ordered_df = df.orderBy(\"userId\")\n",
    "#ordered_df.show()\n",
    "\n",
    "\n",
    "\n",
    "# display(df.select(count(\"*\")).toPandas())\n",
    "display(df.select(\"*\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8597f38-86ac-4e72-ba4d-05b57bf6f006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c590a-b412-4bdb-ab42-f8b7f37944b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6d955-c024-433f-9ead-8ee955bf553d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
